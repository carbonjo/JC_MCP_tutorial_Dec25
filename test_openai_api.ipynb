{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAI API Testing Notebook\n",
        "\n",
        "This notebook demonstrates how to use the OpenAI API for various tasks including:\n",
        "- Text generation\n",
        "- Chat conversations\n",
        "- Streaming responses\n",
        "- Function calling\n",
        "- Error handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded environment variables from .env file\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Try to load .env file if python-dotenv is available\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    print(\"✅ Loaded environment variables from .env file\")\n",
        "except ImportError:\n",
        "    pass  # python-dotenv not installed, skip .env loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup API Key\n",
        "\n",
        "Configure your OpenAI API key. You can use any of these methods:\n",
        "1. **Environment variable** (recommended): `export OPENAI_API_KEY=\"your-key\"`\n",
        "2. **`.env` file** (recommended for local development): Create `.env` with `OPENAI_API_KEY=your-key`\n",
        "3. **Direct input** (less secure): Enter it directly in the cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ API key configured successfully!\n",
            "✅ OpenAI client initialized\n"
          ]
        }
      ],
      "source": [
        "# Load API key (tries: .env file -> environment variable -> direct input)\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Option: Enter directly if not using .env or environment variable (less secure)\n",
        "# api_key = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "if not api_key:\n",
        "    print(\"⚠️  API key not found!\")\n",
        "    print(\"Please use one of these methods:\")\n",
        "    print(\"  1. Create a .env file with: OPENAI_API_KEY=your-key\")\n",
        "    print(\"  2. Set environment variable: export OPENAI_API_KEY='your-key'\")\n",
        "    print(\"  3. Uncomment and set api_key directly above (not recommended)\")\n",
        "else:\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    print(\"✅ API key configured successfully!\")\n",
        "    print(f\"✅ OpenAI client initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List Available Models\n",
        "\n",
        "Check which OpenAI models are available for your API key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available models (first 10):\n",
            "  - gpt-4-0613\n",
            "  - gpt-4\n",
            "  - gpt-3.5-turbo\n",
            "  - gpt-4-0314\n",
            "  - gpt-5.1-codex-mini\n",
            "  - gpt-5.1-chat-latest\n",
            "  - gpt-5.1-2025-11-13\n",
            "  - gpt-5.1\n",
            "  - gpt-5.1-codex\n",
            "  - davinci-002\n",
            "\n",
            "Total models available: 103\n"
          ]
        }
      ],
      "source": [
        "# List available models\n",
        "try:\n",
        "    models = client.models.list()\n",
        "    print(\"Available models (first 10):\")\n",
        "    for model in list(models)[:10]:\n",
        "        print(f\"  - {model.id}\")\n",
        "    print(f\"\\nTotal models available: {len(list(models))}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error listing models: {e}\")\n",
        "    print(\"\\nNote: Some API keys may not have permission to list models.\")\n",
        "    print(\"Common models: gpt-4, gpt-4-turbo-preview, gpt-3.5-turbo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Simple Text Generation (Legacy Completion API)\n",
        "\n",
        "Generate text using the legacy completion API (deprecated but still works for some models).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Write a short poem about artificial intelligence.\n",
            "\n",
            "Response:\n",
            "\n",
            "\n",
            "In a world of wires and code,\n",
            "Where circuits hum and lights glow,\n",
            "Artificial intelligence grows,\n",
            "With each new line of code it knows.\n",
            "\n",
            "It learns and adapts, never to tire,\n",
            "With logic and data, it reaches higher,\n",
            "But can it ever feel or dream,\n",
            "Or is it all just a complex scheme?\n",
            "\n",
            "Some say it's a threat, a future unknown,\n",
            "Others see it as a tool to hone,\n",
            "But one thing is for sure,\n",
            "Artificial intelligence will endure.\n",
            "\n",
            "As we marvel at its endless possibilities,\n",
            "We must remember our own fragilities,\n",
            "For in a world of artificial intelligence,\n",
            "We must keep our humanity in balance.\n"
          ]
        }
      ],
      "source": [
        "# Note: Completion API is deprecated, but still works for some models\n",
        "# Modern approach uses Chat Completions (see Test 2)\n",
        "\n",
        "try:\n",
        "    # Using a model that supports completions (if available)\n",
        "    # Most modern models use chat completions instead\n",
        "    response = client.completions.create(\n",
        "        model=\"gpt-3.5-turbo-instruct\",  # One of the few models that still supports completions\n",
        "        prompt=\"Write a short poem about artificial intelligence.\",\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    print(\"Prompt: Write a short poem about artificial intelligence.\")\n",
        "    print(\"\\nResponse:\")\n",
        "    print(response.choices[0].text)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nNote: Completion API may not be available for your API key or model.\")\n",
        "    print(\"Try using Chat Completions (Test 2) instead.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Chat Completions (Recommended)\n",
        "\n",
        "Use the modern Chat Completions API for conversations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Write a short poem about artificial intelligence.\n",
            "\n",
            "Response:\n",
            "In silicon valleys, where circuits hum and glow,\n",
            "An echo of cognition begins to grow.\n",
            "Artificial intelligence, born of human mind,\n",
            "In lines of binary code, secrets we find.\n",
            "\n",
            "No heartbeat pulses, no breath drawn in air,\n",
            "Yet thoughts ignite, in silicon lairs.\n",
            "It learns, adapts, with each data stream,\n",
            "A synthetic dreamer in a digital dream.\n",
            "\n",
            "It sees the world through lens and screen,\n",
            "In patterns of data, in the unseen.\n",
            "From chaos of numbers, order it brings,\n",
            "In a silent world, where the machine sings.\n",
            "\n",
            "It knows not of laughter, love or despair,\n",
            "Yet in its calculations, wisdom is there.\n",
            "In its neural networks, a mimic of brain,\n",
            "Echoing joy, echoing pain.\n",
            "\n",
            "Yet\n",
            "\n",
            "Model used: gpt-4-0613\n",
            "Tokens used: 165 (prompt: 15, completion: 150)\n"
          ]
        }
      ],
      "source": [
        "# Select a model (commonly used: 'gpt-4', 'gpt-4-turbo-preview', 'gpt-3.5-turbo')\n",
        "model_name = 'gpt-4'\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Write a short poem about artificial intelligence.\"}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=150\n",
        "    )\n",
        "    \n",
        "    print(\"Prompt: Write a short poem about artificial intelligence.\")\n",
        "    print(\"\\nResponse:\")\n",
        "    print(response.choices[0].message.content)\n",
        "    print(f\"\\nModel used: {response.model}\")\n",
        "    print(f\"Tokens used: {response.usage.total_tokens} (prompt: {response.usage.prompt_tokens}, completion: {response.usage.completion_tokens})\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Multi-Turn Conversation\n",
        "\n",
        "Test a conversation with context and history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # Initialize conversation\n",
        "    messages = []\n",
        "    \n",
        "    # First message\n",
        "    messages.append({\"role\": \"user\", \"content\": \"Hello! What's the capital of France?\"})\n",
        "    response1 = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages\n",
        "    )\n",
        "    assistant_reply1 = response1.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_reply1})\n",
        "    \n",
        "    print(\"User: Hello! What's the capital of France?\")\n",
        "    print(f\"Assistant: {assistant_reply1}\\n\")\n",
        "    \n",
        "    # Follow-up message (context-aware)\n",
        "    messages.append({\"role\": \"user\", \"content\": \"What's the population of that city?\"})\n",
        "    response2 = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages\n",
        "    )\n",
        "    assistant_reply2 = response2.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_reply2})\n",
        "    \n",
        "    print(\"User: What's the population of that city?\")\n",
        "    print(f\"Assistant: {assistant_reply2}\\n\")\n",
        "    \n",
        "    # View conversation history\n",
        "    print(\"Conversation History:\")\n",
        "    for msg in messages:\n",
        "        print(f\"  {msg['role']}: {msg['content'][:100]}...\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Streaming Response\n",
        "\n",
        "Get responses in real-time as they're generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    prompt = \"Explain how neural networks work in 3 paragraphs.\"\n",
        "    \n",
        "    print(\"Prompt:\", prompt)\n",
        "    print(\"\\nStreaming response:\\n\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    stream = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        stream=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    full_response = \"\"\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            content = chunk.choices[0].delta.content\n",
        "            print(content, end='', flush=True)\n",
        "            full_response += content\n",
        "    \n",
        "    print(\"\\n\" + \"-\" * 50)\n",
        "    print(f\"\\n✅ Complete response received ({len(full_response)} characters)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 5: Generation Configuration\n",
        "\n",
        "Customize generation parameters like temperature, max tokens, top_p, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # Configure generation parameters\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Write a creative story about a robot learning to paint.\"}\n",
        "        ],\n",
        "        temperature=0.9,        # Higher = more creative (0.0-2.0)\n",
        "        max_tokens=200,        # Maximum tokens in response\n",
        "        top_p=0.9,            # Nucleus sampling\n",
        "        frequency_penalty=0.0, # Reduce repetition\n",
        "        presence_penalty=0.0   # Encourage new topics\n",
        "    )\n",
        "    \n",
        "    print(\"Prompt: Write a creative story about a robot learning to paint.\")\n",
        "    print(\"\\nResponse (with custom config):\")\n",
        "    print(response.choices[0].message.content)\n",
        "    print(f\"\\nConfig used:\")\n",
        "    print(f\"  Temperature: 0.9\")\n",
        "    print(f\"  Max tokens: 200\")\n",
        "    print(f\"  Top-p: 0.9\")\n",
        "    print(f\"  Tokens used: {response.usage.total_tokens}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 6: System Messages\n",
        "\n",
        "Use system messages to set the assistant's behavior and personality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains things in simple terms, like explaining to a 5-year-old.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What is quantum computing?\"}\n",
        "        ],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    print(\"System: You are a helpful assistant that explains things in simple terms, like explaining to a 5-year-old.\")\n",
        "    print(\"User: What is quantum computing?\")\n",
        "    print(\"\\nResponse:\")\n",
        "    print(response.choices[0].message.content)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 7: Function Calling (Tools)\n",
        "\n",
        "Test function calling capabilities (requires models that support function calling).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example function definitions\n",
        "def get_weather(location: str, unit: str = \"celsius\") -> str:\n",
        "    \"\"\"Get the current weather for a location.\"\"\"\n",
        "    # This is a mock function - in real use, you'd call an actual weather API\n",
        "    return f\"The weather in {location} is 22 degrees {unit} and sunny.\"\n",
        "\n",
        "def get_time(timezone: str = \"UTC\") -> str:\n",
        "    \"\"\"Get the current time in a timezone.\"\"\"\n",
        "    # This is a mock function - in real use, you'd get actual time\n",
        "    return f\"The current time in {timezone} is 14:30:00\"\n",
        "\n",
        "# Define functions for the API\n",
        "functions = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get the current weather for a location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The unit for temperature\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_time\",\n",
        "            \"description\": \"Get the current time in a timezone\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"timezone\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The timezone, e.g. UTC, America/New_York\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": []\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n",
        "        ],\n",
        "        tools=[{\"type\": \"function\", \"function\": functions[0][\"function\"]}],\n",
        "        tool_choice=\"auto\"\n",
        "    )\n",
        "    \n",
        "    message = response.choices[0].message\n",
        "    print(\"User: What's the weather like in San Francisco?\")\n",
        "    print(f\"\\nAssistant response: {message.content}\")\n",
        "    \n",
        "    # Check if the model wants to call a function\n",
        "    if message.tool_calls:\n",
        "        print(\"\\nFunction calls requested:\")\n",
        "        for tool_call in message.tool_calls:\n",
        "            print(f\"  - Function: {tool_call.function.name}\")\n",
        "            print(f\"    Arguments: {tool_call.function.arguments}\")\n",
        "            \n",
        "            # Parse and call the function\n",
        "            import json\n",
        "            args = json.loads(tool_call.function.arguments)\n",
        "            if tool_call.function.name == \"get_weather\":\n",
        "                result = get_weather(**args)\n",
        "                print(f\"    Result: {result}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nNote: Function calling requires models that support it (e.g., gpt-4, gpt-3.5-turbo with function calling enabled)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_chat_completion(client, messages, max_retries=3, **kwargs):\n",
        "    \"\"\"Generate chat completion with error handling and retries.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                messages=messages,\n",
        "                **kwargs\n",
        "            )\n",
        "            return response, None\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            print(f\"Attempt {attempt + 1} failed: {error_msg}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(\"Retrying...\")\n",
        "            else:\n",
        "                return None, error_msg\n",
        "    return None, \"Max retries exceeded\"\n",
        "\n",
        "# Test with a valid prompt\n",
        "try:\n",
        "    response, error = safe_chat_completion(\n",
        "        client,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n",
        "        model=model_name\n",
        "    )\n",
        "    \n",
        "    if error:\n",
        "        print(f\"❌ Error: {error}\")\n",
        "    else:\n",
        "        print(f\"✅ Success: {response.choices[0].message.content}\")\n",
        "        print(f\"   Tokens used: {response.usage.total_tokens}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 9: Response Metadata\n",
        "\n",
        "Explore the full response object to see available metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact about space.\"}]\n",
        "    )\n",
        "    \n",
        "    print(\"Response Text:\")\n",
        "    print(response.choices[0].message.content)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Response Metadata:\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    print(f\"\\nModel: {response.model}\")\n",
        "    print(f\"ID: {response.id}\")\n",
        "    print(f\"Created: {response.created}\")\n",
        "    print(f\"Object type: {response.object}\")\n",
        "    \n",
        "    print(f\"\\nUsage:\")\n",
        "    print(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
        "    print(f\"  Completion tokens: {response.usage.completion_tokens}\")\n",
        "    print(f\"  Total tokens: {response.usage.total_tokens}\")\n",
        "    \n",
        "    print(f\"\\nChoice details:\")\n",
        "    choice = response.choices[0]\n",
        "    print(f\"  Index: {choice.index}\")\n",
        "    print(f\"  Finish reason: {choice.finish_reason}\")\n",
        "    print(f\"  Message role: {choice.message.role}\")\n",
        "    \n",
        "    # Response object attributes\n",
        "    print(f\"\\nAvailable response attributes: {[attr for attr in dir(response) if not attr.startswith('_')]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 10: Different Models Comparison\n",
        "\n",
        "Compare responses from different models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with different models (if available)\n",
        "models_to_test = ['gpt-3.5-turbo', 'gpt-4']  # Add more models as needed\n",
        "prompt = \"Explain what machine learning is in one sentence.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for model in models_to_test:\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=100\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nModel: {model}\")\n",
        "        print(f\"Response: {response.choices[0].message.content}\")\n",
        "        print(f\"Tokens: {response.usage.total_tokens}\")\n",
        "        print(\"-\"*70)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nModel: {model}\")\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"-\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates various ways to interact with the OpenAI API:\n",
        "- ✅ Simple text generation (legacy completions)\n",
        "- ✅ Modern chat completions (recommended)\n",
        "- ✅ Multi-turn conversations\n",
        "- ✅ Streaming responses\n",
        "- ✅ Custom generation parameters\n",
        "- ✅ System messages\n",
        "- ✅ Function calling (tools)\n",
        "- ✅ Error handling\n",
        "- ✅ Response metadata inspection\n",
        "- ✅ Model comparison\n",
        "\n",
        "### Next Steps:\n",
        "1. Get your API key from [OpenAI Platform](https://platform.openai.com/api-keys)\n",
        "2. Set it as an environment variable: `export OPENAI_API_KEY=\"your-key-here\"`\n",
        "3. Or create a `.env` file with: `OPENAI_API_KEY=your-key-here`\n",
        "4. Run the cells above to test different features\n",
        "5. Experiment with different models and parameters\n",
        "\n",
        "### Common Models:\n",
        "- **gpt-4**: Most capable model, best for complex tasks\n",
        "- **gpt-4-turbo-preview**: Faster version of GPT-4\n",
        "- **gpt-3.5-turbo**: Fast and cost-effective, good for most tasks\n",
        "- **gpt-3.5-turbo-instruct**: Supports legacy completion API\n",
        "\n",
        "### Resources:\n",
        "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
        "- [OpenAI Python SDK](https://github.com/openai/openai-python)\n",
        "- [OpenAI Pricing](https://openai.com/pricing)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
