{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MCP Agents with LLM Integration\n",
        "\n",
        "**ADVANCED NOTEBOOK** - Run `mcp_client_demo.ipynb` first to understand the basics!\n",
        "\n",
        "This notebook demonstrates how to use LLMs (Large Language Models) with MCP servers to create intelligent agents that can:\n",
        "- Understand natural language requests\n",
        "- Decide which MCP tools to use\n",
        "- Execute actions through MCP servers\n",
        "- Provide intelligent responses\n",
        "\n",
        "## Two Integration Methods:\n",
        "\n",
        "1. **Ollama (Local Models)**: Run models locally without API keys\n",
        "2. **API-based Models**: Use OpenAI or Google Gemini with API keys\n",
        "\n",
        "## Setup\n",
        "\n",
        "Make sure you have:\n",
        "- ‚úÖ Completed `mcp_client_demo.ipynb` first\n",
        "- ‚úÖ Installed dependencies: `pip install -r requirements.txt`\n",
        "- ‚úÖ For Ollama: Install and run Ollama locally (https://ollama.ai)\n",
        "- ‚úÖ For API models: Set up API keys in `.env` file (see `API_KEYS_SETUP.md`)\n",
        "\n",
        "### API Keys Setup (Cloud Models)\n",
        "\n",
        "**No interactive prompts!** API keys are read from `.env` file:\n",
        "\n",
        "1. Create `.env` file in project root\n",
        "2. Add your keys:\n",
        "   ```\n",
        "   OPENAI_API_KEY=your-openai-key-here\n",
        "   GEMINI_API_KEY=your-gemini-key-here\n",
        "   ```\n",
        "3. Get keys from:\n",
        "   - OpenAI: https://platform.openai.com/api-keys\n",
        "   - Gemini: https://aistudio.google.com/app/apikey\n",
        "\n",
        "See `API_KEYS_SETUP.md` for detailed instructions.\n",
        "\n",
        "## How to Use\n",
        "\n",
        "1. **Run cells ONE AT A TIME**\n",
        "2. **Wait for each cell to complete** (look for `[1]`, `[2]`, etc.)\n",
        "3. **`[*]` means running** - wait for it to finish\n",
        "4. **If stuck**: Press `Ctrl+C` (or `Cmd+C`) to interrupt, then restart kernel\n",
        "\n",
        "## Note: Execution Numbers and Timestamps\n",
        "\n",
        "If you don't see cell execution numbers or timestamps:\n",
        "- **JupyterLab**: Go to View ‚Üí Show Line Numbers, or Settings ‚Üí Advanced Settings Editor ‚Üí Notebook ‚Üí enable `showExecutionTime`\n",
        "- **Jupyter Notebook**: Execution numbers should appear automatically when you run cells\n",
        "- **VS Code**: Check Jupyter extension settings for execution display options\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Loaded .env from: /Users/carbonjo/Library/CloudStorage/Dropbox/AI/Agents-Langchain-llamaindex-MCP/MCP_Nov29-25/.env\n",
            "Using Python: /Users/carbonjo/Library/CloudStorage/Dropbox/AI/Agents-Langchain-llamaindex-MCP/MCP_Nov29-25/venv/bin/python\n",
            "‚úì OPENAI_API_KEY loaded: sk-proj-Ih...12wA (length: 164)\n",
            "‚úì GEMINI_API_KEY loaded: AIzaSyDzWO...BRI4 (length: 39)\n",
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "# Load environment variables from .env file FIRST (before other imports that might need them)\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Try multiple paths for .env file\n",
        "# 1. Try project root (parent of notebooks/)\n",
        "env_paths = [\n",
        "    Path(\"../.env\").resolve(),  # From notebooks/ directory\n",
        "    Path(\".env\").resolve(),     # Current working directory\n",
        "    Path(__file__).parent.parent / \".env\" if '__file__' in globals() else None,  # Absolute from file\n",
        "]\n",
        "\n",
        "# Also try loading from current directory (like test notebook does)\n",
        "load_dotenv()  # This loads from current working directory\n",
        "\n",
        "# Then try explicit paths\n",
        "for env_path in env_paths:\n",
        "    if env_path and env_path.exists():\n",
        "        load_dotenv(env_path, override=False)  # Don't override if already loaded\n",
        "        print(f\"‚úì Loaded .env from: {env_path}\")\n",
        "        break\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  No .env file found - using environment variables only\")\n",
        "\n",
        "# MCP imports\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "\n",
        "# Get the Python interpreter from the current environment (venv)\n",
        "# This ensures servers use the same Python that has MCP installed\n",
        "PYTHON_EXECUTABLE = sys.executable\n",
        "print(f\"Using Python: {PYTHON_EXECUTABLE}\")\n",
        "\n",
        "# Verify API keys are loaded (for debugging)\n",
        "if os.getenv(\"OPENAI_API_KEY\"):\n",
        "    key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    print(f\"‚úì OPENAI_API_KEY loaded: {key[:10]}...{key[-4:]} (length: {len(key)})\")\n",
        "if os.getenv(\"GEMINI_API_KEY\"):\n",
        "    key = os.getenv(\"GEMINI_API_KEY\")\n",
        "    print(f\"‚úì GEMINI_API_KEY loaded: {key[:10]}...{key[-4:]} (length: {len(key)})\")\n",
        "\n",
        "# LLM imports\n",
        "try:\n",
        "    import ollama\n",
        "    OLLAMA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OLLAMA_AVAILABLE = False\n",
        "    print(\"Ollama not installed. Install with: pip install ollama\")\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    OPENAI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPENAI_AVAILABLE = False\n",
        "    print(\"OpenAI not installed. Install with: pip install openai\")\n",
        "\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    GEMINI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GEMINI_AVAILABLE = False\n",
        "    print(\"Google Generative AI not installed. Install with: pip install google-generativeai\")\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions: MCP Server Connections\n",
        "\n",
        "These functions help connect to our MCP servers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading MCP server tools...\n",
            "(This may take a moment as servers start up)\n",
            "\n",
            "‚úì code server: 5 tools\n",
            "‚úì database server: 5 tools\n",
            "‚úì document server: 6 tools\n",
            "\n",
            "‚úì Total tools available: 16\n"
          ]
        }
      ],
      "source": [
        "# MCP Server configurations\n",
        "# Note: Servers are automatically spawned when we connect\n",
        "# IMPORTANT: Use PYTHON_EXECUTABLE to ensure servers use the venv Python with MCP installed\n",
        "CODE_SERVER_PARAMS = StdioServerParameters(\n",
        "    command=PYTHON_EXECUTABLE,  # Use venv Python, not system Python\n",
        "    args=[str(Path(\"../servers/code_server.py\").absolute())],\n",
        "    env=None\n",
        ")\n",
        "\n",
        "DB_SERVER_PARAMS = StdioServerParameters(\n",
        "    command=PYTHON_EXECUTABLE,  # Use venv Python, not system Python\n",
        "    args=[str(Path(\"../servers/database_server.py\").absolute())],\n",
        "    env=None\n",
        ")\n",
        "\n",
        "DOC_SERVER_PARAMS = StdioServerParameters(\n",
        "    command=PYTHON_EXECUTABLE,  # Use venv Python, not system Python\n",
        "    args=[str(Path(\"../servers/document_server.py\").absolute())],\n",
        "    env=None\n",
        ")\n",
        "\n",
        "async def get_mcp_tools(server_params):\n",
        "    \"\"\"Get all available tools from an MCP server.\"\"\"\n",
        "    try:\n",
        "        async with stdio_client(server_params) as (read, write):\n",
        "            async with ClientSession(read, write) as session:\n",
        "                await session.initialize()\n",
        "                tools_response = await session.list_tools()\n",
        "                return {tool.name: tool for tool in tools_response.tools}\n",
        "    except Exception as e:\n",
        "        print(f\"Error connecting to server: {e}\")\n",
        "        return {}\n",
        "\n",
        "async def call_mcp_tool(server_params, tool_name, arguments):\n",
        "    \"\"\"Call a tool on an MCP server.\"\"\"\n",
        "    try:\n",
        "        async with stdio_client(server_params) as (read, write):\n",
        "            async with ClientSession(read, write) as session:\n",
        "                await session.initialize()\n",
        "                result = await session.call_tool(tool_name, arguments)\n",
        "                return result.content[0].text if result.content else \"No output\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calling tool: {str(e)}\"\n",
        "\n",
        "# Get all available tools from all servers\n",
        "print(\"Loading MCP server tools...\")\n",
        "print(\"(This may take a moment as servers start up)\\n\")\n",
        "all_tools = {}\n",
        "for name, params in [(\"code\", CODE_SERVER_PARAMS), (\"database\", DB_SERVER_PARAMS), (\"document\", DOC_SERVER_PARAMS)]:\n",
        "    try:\n",
        "        tools = await get_mcp_tools(params)\n",
        "        all_tools.update({f\"{name}_{k}\": v for k, v in tools.items()})\n",
        "        print(f\"‚úì {name} server: {len(tools)} tools\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó {name} server: Error - {e}\")\n",
        "\n",
        "if all_tools:\n",
        "    print(f\"\\n‚úì Total tools available: {len(all_tools)}\")\n",
        "else:\n",
        "    print(\"\\n‚ö† Warning: No tools loaded. Check server paths and MCP installation.\")\n",
        "    print(\"Troubleshooting:\")\n",
        "    print(\"1. Make sure you're running from the notebooks/ directory\")\n",
        "    print(\"2. Check that server files exist in ../servers/\")\n",
        "    print(\"3. Verify MCP is installed: pip install mcp\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 1: Ollama Integration (Local Models)\n",
        "\n",
        "Ollama allows you to run LLMs locally without API keys. First, make sure Ollama is installed and running.\n",
        "\n",
        "### Setup Ollama\n",
        "\n",
        "1. Install Ollama: https://ollama.ai\n",
        "2. Pull a model: `ollama pull llama3` (or any other model)\n",
        "3. Start Ollama service: `ollama serve`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Ollama is running!\n",
            "Available models: ['llama3.2-vision:latest', 'gemma3:4b', 'llama4:latest', 'llama3.2-vision:11b', 'llama3.2:latest', 'llama3.1:latest', 'mistral:latest', 'llama3:latest']\n"
          ]
        }
      ],
      "source": [
        "def create_tools_description():\n",
        "    \"\"\"Create a description of all available MCP tools for the LLM.\"\"\"\n",
        "    description = \"Available MCP Tools:\\n\\n\"\n",
        "    for tool_name, tool in all_tools.items():\n",
        "        description += f\"- {tool_name}: {tool.description}\\n\"\n",
        "        if hasattr(tool, 'inputSchema') and 'properties' in tool.inputSchema:\n",
        "            props = tool.inputSchema['properties']\n",
        "            description += \"  Parameters: \" + \", \".join(props.keys()) + \"\\n\"\n",
        "        description += \"\\n\"\n",
        "    return description\n",
        "\n",
        "def ollama_agent(prompt: str, model: str = \"llama3\", system_prompt: Optional[str] = None):\n",
        "    \"\"\"Use Ollama to process a prompt and decide on MCP tool usage.\"\"\"\n",
        "    if not OLLAMA_AVAILABLE:\n",
        "        return \"Ollama is not available. Please install it: pip install ollama\"\n",
        "    \n",
        "    tools_desc = create_tools_description()\n",
        "    \n",
        "    if system_prompt is None:\n",
        "        system_prompt = f\"\"\"You are an AI agent that can use MCP (Model Context Protocol) tools to help users.\n",
        "\n",
        "{tools_desc}\n",
        "\n",
        "When a user asks you to do something, analyze the request and determine:\n",
        "1. Which tool(s) to use\n",
        "2. What parameters to pass\n",
        "3. How to interpret the results\n",
        "\n",
        "Respond in JSON format with:\n",
        "{{\n",
        "    \"reasoning\": \"Your thought process\",\n",
        "    \"tool\": \"tool_name\",\n",
        "    \"server\": \"code|database|document\",\n",
        "    \"arguments\": {{\"param1\": \"value1\", \"param2\": \"value2\"}},\n",
        "    \"response\": \"What to tell the user\"\n",
        "}}\n",
        "\n",
        "If multiple tools are needed, provide a list of actions.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        return response['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"Error calling Ollama: {str(e)}\"\n",
        "\n",
        "# Test Ollama connection\n",
        "if OLLAMA_AVAILABLE:\n",
        "    try:\n",
        "        test_response = ollama.list()\n",
        "        print(\"‚úì Ollama is running!\")\n",
        "        # Ollama API returns a ListResponse object, not a dict\n",
        "        # Access models as an attribute, and model name as m.model\n",
        "        if hasattr(test_response, 'models'):\n",
        "            models = [m.model for m in test_response.models]\n",
        "            print(f\"Available models: {models}\")\n",
        "        else:\n",
        "            # Fallback for older API versions\n",
        "            models = [m['name'] for m in test_response.get('models', [])]\n",
        "            print(f\"Available models: {models}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Ollama connection error: {e}\")\n",
        "        print(\"Make sure Ollama is running: ollama serve\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"‚ö† Ollama not installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User request: \n",
            "    Create a document called 'test' with content the summary of \n",
            "    the book The Prince by Machiavelli\n",
            "\n",
            "LLM Analysis:\n",
            "```json\n",
            "{\n",
            "    \"reasoning\": \"The user wants to create a document. I need to use the document_create_document tool to achieve this. The content of the document should be a summary of \\\"The Prince\\\" by Machiavelli.\",\n",
            "    \"tool\": \"document_create_document\",\n",
            "    \"server\": \"document\",\n",
            "    \"arguments\": {\n",
            "        \"name\": \"test\",\n",
            "        \"content\": \"Niccol√≤ Machiavelli‚Äôs *The Prince* is a political treatise that outlines a pragmatic and often ruthless approach to acquiring and maintaining power. Written in the early 16th century, it argues that a ruler must prioritize the stability and security of the state above all else, even if this requires deception, manipulation, and the use of violence. Machiavelli distinguishes between virtue and vice, suggesting that a prince should be feared rather than loved if he cannot be both. The book emphasizes the importance of appearances and the skillful use of propaganda. It is a controversial work, seen as cynical by some and a realistic assessment of political realities by others.\"\n",
            "    },\n",
            "    \"response\": \"I have created a document called 'test' with the content: \\\"Niccol√≤ Machiavelli‚Äôs *The Prince* is a political treatise that outlines a pragmatic and often ruthless approach to acquiring and maintaining power. Written in the early 16th century, it argues that a ruler must prioritize the stability and security of the state above all else, even if this requires deception, manipulation, and the use of violence. Machiavelli distinguishes between virtue and vice, suggesting that a prince should be feared rather than loved if he cannot be both. The book emphasizes the importance of appearances and the skillful use of propaganda. It is a controversial work, seen as cynical by some and a realistic assessment of political realities by others.\\\"\"\n",
            "}\n",
            "```\n",
            "\n",
            "Executing: document.create_document with arguments: {'name': 'test', 'content': 'Niccol√≤ Machiavelli‚Äôs *The Prince* is a political treatise that outlines a pragmatic and often ruthless approach to acquiring and maintaining power. Written in the early 16th century, it argues that a ruler must prioritize the stability and security of the state above all else, even if this requires deception, manipulation, and the use of violence. Machiavelli distinguishes between virtue and vice, suggesting that a prince should be feared rather than loved if he cannot be both. The book emphasizes the importance of appearances and the skillful use of propaganda. It is a controversial work, seen as cynical by some and a realistic assessment of political realities by others.'}\n",
            "\n",
            "Tool result:\n",
            "Document 'test' created successfully (682 characters)\n",
            "\n",
            "Final result: Document 'test' created successfully (682 characters)\n"
          ]
        }
      ],
      "source": [
        "async def execute_ollama_agent_request(user_prompt: str, model: str = \"gemma3:4b\"):\n",
        "    \"\"\"Execute a user request using Ollama agent.\"\"\"\n",
        "    print(f\"User request: {user_prompt}\\n\")\n",
        "    \n",
        "    # Get LLM response\n",
        "    llm_response = ollama_agent(user_prompt, model=model)\n",
        "    print(f\"LLM Analysis:\\n{llm_response}\\n\")\n",
        "    \n",
        "    # Try to parse JSON response\n",
        "    try:\n",
        "        # Extract JSON from response (might be wrapped in markdown)\n",
        "        import re\n",
        "        json_match = re.search(r'\\{.*\\}', llm_response, re.DOTALL)\n",
        "        if json_match:\n",
        "            decision = json.loads(json_match.group())\n",
        "        else:\n",
        "            print(\"Could not parse JSON from LLM response. Using direct interpretation.\")\n",
        "            decision = {\"response\": llm_response, \"tool\": None}\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing LLM response: {e}\")\n",
        "        decision = {\"response\": llm_response, \"tool\": None}\n",
        "    \n",
        "    # Execute tool if specified\n",
        "    # Handle both single tool and list of tools\n",
        "    tool_data = decision.get(\"tool\")\n",
        "    server = decision.get(\"server\")\n",
        "    arguments = decision.get(\"arguments\")\n",
        "    \n",
        "    # If tool is a list, take the first one (or handle multiple tools)\n",
        "    if isinstance(tool_data, list):\n",
        "        if len(tool_data) > 0:\n",
        "            tool_name = tool_data[0]\n",
        "            print(f\"‚ö† LLM returned multiple tools, using first: {tool_name}\")\n",
        "        else:\n",
        "            return decision.get(\"response\", \"No action taken\")\n",
        "    elif tool_data:\n",
        "        tool_name = tool_data\n",
        "    else:\n",
        "        return decision.get(\"response\", \"No action taken\")\n",
        "    \n",
        "    # Validate we have all required fields\n",
        "    if not tool_name or not server or not arguments:\n",
        "        return decision.get(\"response\", \"No action taken - missing tool, server, or arguments\")\n",
        "    \n",
        "    # Ensure tool_name is a string\n",
        "    if not isinstance(tool_name, str):\n",
        "        tool_name = str(tool_name)\n",
        "    \n",
        "    # Strip server prefix from tool name if present (e.g., \"document_create_document\" -> \"create_document\")\n",
        "    # The LLM might return the prefixed name, but we need the actual tool name\n",
        "    if isinstance(tool_name, str) and tool_name.startswith(f\"{server}_\"):\n",
        "        actual_tool_name = tool_name[len(f\"{server}_\"):]\n",
        "    else:\n",
        "        actual_tool_name = tool_name\n",
        "    \n",
        "    # Map server name to params\n",
        "    server_map = {\n",
        "        \"code\": CODE_SERVER_PARAMS,\n",
        "        \"database\": DB_SERVER_PARAMS,\n",
        "        \"document\": DOC_SERVER_PARAMS\n",
        "    }\n",
        "    \n",
        "    if server in server_map:\n",
        "        print(f\"Executing: {server}.{actual_tool_name} with arguments: {arguments}\\n\")\n",
        "        try:\n",
        "            result = await call_mcp_tool(server_map[server], actual_tool_name, arguments)\n",
        "            print(f\"Tool result:\\n{result}\\n\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error executing tool: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            return error_msg\n",
        "    else:\n",
        "        return f\"Unknown server: {server}\"\n",
        "\n",
        "# Example: Use Ollama agent\n",
        "if OLLAMA_AVAILABLE:\n",
        "    # Uncomment to test:\n",
        "    result = await execute_ollama_agent_request('''\n",
        "    Create a document called 'test' with content the summary of \n",
        "    the book The Prince by Machiavelli''')#'Hello from Ollama agent!'\")\n",
        "    print(f\"Final result: {result}\")\n",
        "    #print(\"Ready to use Ollama agent. Uncomment the example above to test.\")\n",
        "else:\n",
        "    print(\"Ollama not available. Install with: pip install ollama\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 2: API-based Models (OpenAI & Gemini)\n",
        "\n",
        "This method uses cloud-based LLM APIs. You'll need to provide API keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Loaded .env file from: /Users/carbonjo/Library/CloudStorage/Dropbox/AI/Agents-Langchain-llamaindex-MCP/MCP_Nov29-25\n",
            "============================================================\n",
            "üîë API Key Setup\n",
            "============================================================\n",
            "API keys are read from .env file or environment variables.\n",
            "\n",
            "To set up API keys:\n",
            "  1. Create a .env file in the project root\n",
            "  2. Add your keys:\n",
            "     OPENAI_API_KEY=your-openai-key-here\n",
            "     GEMINI_API_KEY=your-gemini-key-here\n",
            "\n",
            "üí° Tip: You only need to set up the providers you want to use.\n",
            "   Ollama works without any keys (local models).\n",
            "============================================================\n",
            "\n",
            "‚úì Found OPENAI API key: sk-p************************************************************************************************************************************************************52gA\n",
            "‚úì OpenAI client initialized\n",
            "\n",
            "‚úì Found GEMINI API key: AIza*******************************BRI4\n",
            "   Checking available Gemini models...\n",
            "   Found 41 available model(s)\n",
            "‚úì Gemini client initialized (using gemini-2.5-flash)\n",
            "\n",
            "============================================================\n",
            "‚úÖ Setup Complete!\n",
            "============================================================\n",
            "üìã Available providers: Ollama (local), OpenAI, Gemini (gemini-2.5-flash)\n",
            "\n",
            "üí° Use these providers in your agent calls:\n",
            "   ‚Ä¢ provider='openai'\n",
            "   ‚Ä¢ provider='gemini'\n",
            "   ‚Ä¢ provider='ollama'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# API Key Configuration\n",
        "# API keys are read from environment variables or .env file\n",
        "# The .env file is already loaded in cell 1 above\n",
        "# Create a .env file in the project root with:\n",
        "#   OPENAI_API_KEY=your-openai-key-here\n",
        "#   GEMINI_API_KEY=your-gemini-key-here\n",
        "\n",
        "# Note: .env file should already be loaded from cell 1\n",
        "# This cell just reads the keys that were loaded\n",
        "\n",
        "def get_api_key(provider: str) -> Optional[str]:\n",
        "    \"\"\"Get API key from environment variable or .env file.\"\"\"\n",
        "    env_key = os.getenv(f\"{provider.upper()}_API_KEY\")\n",
        "    if env_key:\n",
        "        # Mask the key for display (show only first/last few characters)\n",
        "        if len(env_key) > 8:\n",
        "            masked = env_key[:4] + \"*\" * (len(env_key) - 8) + env_key[-4:]\n",
        "        else:\n",
        "            masked = \"*\" * len(env_key)\n",
        "        print(f\"‚úì Found {provider.upper()} API key: {masked}\")\n",
        "        return env_key\n",
        "    else:\n",
        "        print(f\"‚ö† {provider.upper()} API key not found\")\n",
        "        print(f\"   Set it in .env file or environment variable: {provider.upper()}_API_KEY\")\n",
        "        if provider.lower() == \"openai\":\n",
        "            print(f\"   Get your key from: https://platform.openai.com/api-keys\")\n",
        "        elif provider.lower() == \"gemini\":\n",
        "            print(f\"   Get your key from: https://aistudio.google.com/app/apikey\")\n",
        "        return None\n",
        "\n",
        "# Initialize API clients\n",
        "openai_client = None\n",
        "gemini_client = None\n",
        "gemini_model_name = None  # Store which model worked\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üîë API Key Setup\")\n",
        "print(\"=\"*60)\n",
        "print(\"API keys are read from .env file or environment variables.\")\n",
        "print(\"\\nTo set up API keys:\")\n",
        "print(\"  1. Create a .env file in the project root\")\n",
        "print(\"  2. Add your keys:\")\n",
        "print(\"     OPENAI_API_KEY=your-openai-key-here\")\n",
        "print(\"     GEMINI_API_KEY=your-gemini-key-here\")\n",
        "print(\"\\nüí° Tip: You only need to set up the providers you want to use.\")\n",
        "print(\"   Ollama works without any keys (local models).\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "# OpenAI Setup\n",
        "if OPENAI_AVAILABLE:\n",
        "    api_key = get_api_key(\"openai\")\n",
        "    if api_key:\n",
        "        try:\n",
        "            # Test the API key by making a simple call\n",
        "            test_client = OpenAI(api_key=api_key)\n",
        "            # Don't make an actual API call during setup, just create the client\n",
        "            openai_client = test_client\n",
        "            print(\"‚úì OpenAI client initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† Error initializing OpenAI: {e}\")\n",
        "            print(\"   The API key might be invalid. Check your .env file.\")\n",
        "            openai_client = None\n",
        "    else:\n",
        "        print(\"‚è≠Ô∏è  OpenAI skipped (you can set it up later if needed)\")\n",
        "else:\n",
        "    print(\"‚ö† OpenAI library not installed\")\n",
        "\n",
        "print()  # Add spacing\n",
        "\n",
        "# Gemini Setup\n",
        "if GEMINI_AVAILABLE:\n",
        "    api_key = get_api_key(\"gemini\")\n",
        "    if api_key:\n",
        "        genai.configure(api_key=api_key)\n",
        "        gemini_client = None\n",
        "        gemini_model_name = None\n",
        "        \n",
        "        # First, try to list available models to find the right one\n",
        "        try:\n",
        "            print(\"   Checking available Gemini models...\")\n",
        "            available_models = []\n",
        "            model_names_with_prefix = []\n",
        "            for m in genai.list_models():\n",
        "                if 'generateContent' in m.supported_generation_methods:\n",
        "                    full_name = m.name  # e.g., \"models/gemini-2.5-flash\"\n",
        "                    short_name = full_name.replace('models/', '')  # e.g., \"gemini-2.5-flash\"\n",
        "                    available_models.append(short_name)\n",
        "                    model_names_with_prefix.append((full_name, short_name))\n",
        "            \n",
        "            if available_models:\n",
        "                print(f\"   Found {len(available_models)} available model(s)\")\n",
        "                # Try models in order of preference (newer models first)\n",
        "                preferred_models = [\n",
        "                    'gemini-2.5-flash',      # Latest flash\n",
        "                    'gemini-2.5-pro',       # Latest pro\n",
        "                    'gemini-2.0-flash',     # Previous version\n",
        "                    'gemini-1.5-flash',     # Older but stable\n",
        "                    'gemini-1.5-pro',       # Older but stable\n",
        "                    'gemini-pro',           # Classic\n",
        "                    'gemini-pro-latest'      # Latest alias\n",
        "                ]\n",
        "                \n",
        "                # Try preferred models first\n",
        "                for preferred in preferred_models:\n",
        "                    if preferred in available_models:\n",
        "                        try:\n",
        "                            # Try with and without models/ prefix\n",
        "                            for full_name, short_name in model_names_with_prefix:\n",
        "                                if short_name == preferred:\n",
        "                                    test_model = genai.GenerativeModel(full_name)\n",
        "                                    gemini_client = test_model\n",
        "                                    gemini_model_name = short_name  # Store without prefix for consistency\n",
        "                                    print(f\"‚úì Gemini client initialized (using {short_name})\")\n",
        "                                    break\n",
        "                            if gemini_client:\n",
        "                                break\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "                \n",
        "                # If preferred models didn't work, try first available\n",
        "                if gemini_client is None:\n",
        "                    try:\n",
        "                        first_full, first_short = model_names_with_prefix[0]\n",
        "                        gemini_client = genai.GenerativeModel(first_full)\n",
        "                        gemini_model_name = first_short\n",
        "                        print(f\"‚úì Gemini client initialized (using first available: {first_short})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö† Could not initialize with any model: {e}\")\n",
        "                        print(f\"   Sample available models: {', '.join(available_models[:5])}\")\n",
        "                        gemini_client = None\n",
        "            else:\n",
        "                print(\"‚ö† No models found with generateContent support\")\n",
        "                gemini_client = None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† Error checking Gemini models: {e}\")\n",
        "            print(\"   Trying default model names...\")\n",
        "            # Fallback to trying common model names\n",
        "            models_to_try = ['gemini-pro', 'gemini-1.5-pro', 'gemini-1.5-flash']\n",
        "            for model_name in models_to_try:\n",
        "                try:\n",
        "                    test_model = genai.GenerativeModel(model_name)\n",
        "                    gemini_client = test_model\n",
        "                    gemini_model_name = model_name\n",
        "                    print(f\"‚úì Gemini client initialized (using {model_name})\")\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "            \n",
        "            if gemini_client is None:\n",
        "                print(\"‚ö† Could not initialize Gemini client\")\n",
        "                print(\"   Check your API key and available models at: https://ai.google.dev/gemini-api/docs/models/gemini\")\n",
        "    else:\n",
        "        print(\"‚è≠Ô∏è  Gemini skipped (you can set it up later if needed)\")\n",
        "        gemini_client = None\n",
        "        gemini_model_name = None\n",
        "else:\n",
        "    print(\"‚ö† Gemini library not installed\")\n",
        "    gemini_client = None\n",
        "    gemini_model_name = None\n",
        "\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Setup Complete!\")\n",
        "print(\"=\"*60)\n",
        "# Show summary\n",
        "providers_available = []\n",
        "if OLLAMA_AVAILABLE:\n",
        "    providers_available.append(\"Ollama (local)\")\n",
        "if openai_client:\n",
        "    providers_available.append(\"OpenAI\")\n",
        "if gemini_client:\n",
        "    providers_available.append(f\"Gemini ({gemini_model_name if gemini_model_name else 'default'})\")\n",
        "\n",
        "if providers_available:\n",
        "    print(f\"üìã Available providers: {', '.join(providers_available)}\")\n",
        "    print(\"\\nüí° Use these providers in your agent calls:\")\n",
        "    if openai_client:\n",
        "        print(\"   ‚Ä¢ provider='openai'\")\n",
        "    if gemini_client:\n",
        "        print(\"   ‚Ä¢ provider='gemini'\")\n",
        "    if OLLAMA_AVAILABLE:\n",
        "        print(\"   ‚Ä¢ provider='ollama'\")\n",
        "else:\n",
        "    print(\"‚ö† No providers available. Set up at least one above, or use Ollama (local, no key needed).\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def openai_agent(prompt: str, model: str = \"gpt-4o-mini\", system_prompt: Optional[str] = None):\n",
        "    \"\"\"Use OpenAI to process a prompt and decide on MCP tool usage.\"\"\"\n",
        "    # If client not initialized, try to get key and create it\n",
        "    if not openai_client:\n",
        "        api_key = get_api_key(\"openai\")\n",
        "        if not api_key:\n",
        "            return \"OpenAI client not initialized. Please provide an API key in .env file.\"\n",
        "        # Try to recreate the client\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            global openai_client\n",
        "            openai_client = OpenAI(api_key=api_key)\n",
        "        except Exception as e:\n",
        "            return f\"OpenAI client initialization failed: {e}\"\n",
        "    \n",
        "    tools_desc = create_tools_description()\n",
        "    \n",
        "    if system_prompt is None:\n",
        "        system_prompt = f\"\"\"You are an AI agent that can use MCP (Model Context Protocol) tools to help users.\n",
        "\n",
        "{tools_desc}\n",
        "\n",
        "When a user asks you to do something, analyze the request and determine:\n",
        "1. Which tool(s) to use\n",
        "2. What parameters to pass\n",
        "3. How to interpret the results\n",
        "\n",
        "Respond in JSON format with:\n",
        "{{\n",
        "    \"reasoning\": \"Your thought process\",\n",
        "    \"tool\": \"tool_name\",\n",
        "    \"server\": \"code|database|document\",\n",
        "    \"arguments\": {{\"param1\": \"value1\", \"param2\": \"value2\"}},\n",
        "    \"response\": \"What to tell the user\"\n",
        "}}\n",
        "\n",
        "If multiple tools are needed, provide a list of actions.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"} if \"gpt-4\" in model or \"gpt-3.5\" in model else None\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        # Provide helpful error messages\n",
        "        if \"401\" in error_msg or \"invalid_api_key\" in error_msg.lower():\n",
        "            # Check what key is being used for debugging\n",
        "            api_key_check = get_api_key(\"openai\")\n",
        "            if api_key_check:\n",
        "                key_preview = f\"{api_key_check[:10]}...{api_key_check[-4:]}\"\n",
        "                return f\"‚ùå OpenAI API key is invalid or expired.\\n   Current key: {key_preview}\\n   Please check your .env file and get a new key from: https://platform.openai.com/api-keys\\n   Note: Even if the key loads successfully, it may be invalid or expired.\"\n",
        "            else:\n",
        "                return f\"‚ùå OpenAI API key not found. Please add OPENAI_API_KEY to your .env file.\\n   Get a key from: https://platform.openai.com/api-keys\"\n",
        "        elif \"429\" in error_msg or \"rate_limit\" in error_msg.lower():\n",
        "            return f\"‚ùå OpenAI rate limit exceeded. Please wait a moment and try again.\"\n",
        "        else:\n",
        "            return f\"‚ùå Error calling OpenAI: {error_msg}\"\n",
        "\n",
        "def gemini_agent(prompt: str, model: str = None, system_prompt: Optional[str] = None):\n",
        "    \"\"\"Use Google Gemini to process a prompt and decide on MCP tool usage.\"\"\"\n",
        "    if not gemini_client:\n",
        "        return \"Gemini client not initialized. Please provide an API key.\"\n",
        "    \n",
        "    # Use the model that worked during initialization, or the provided model\n",
        "    if model is None:\n",
        "        model = gemini_model_name if gemini_model_name else \"gemini-pro\"\n",
        "    \n",
        "    tools_desc = create_tools_description()\n",
        "    \n",
        "    if system_prompt is None:\n",
        "        system_prompt = f\"\"\"You are an AI agent that can use MCP (Model Context Protocol) tools to help users.\n",
        "\n",
        "{tools_desc}\n",
        "\n",
        "When a user asks you to do something, analyze the request and determine:\n",
        "1. Which tool(s) to use\n",
        "2. What parameters to pass\n",
        "3. How to interpret the results\n",
        "\n",
        "Respond in JSON format with:\n",
        "{{\n",
        "    \"reasoning\": \"Your thought process\",\n",
        "    \"tool\": \"tool_name\",\n",
        "    \"server\": \"code|database|document\",\n",
        "    \"arguments\": {{\"param1\": \"value1\", \"param2\": \"value2\"}},\n",
        "    \"response\": \"What to tell the user\"\n",
        "}}\n",
        "\n",
        "If multiple tools are needed, provide a list of actions.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Create model instance with the specified model name\n",
        "        # Use the model that worked during initialization, or try the specified one\n",
        "        if model and model != gemini_model_name:\n",
        "            try:\n",
        "                # Try with models/ prefix first, then without\n",
        "                try:\n",
        "                    model_client = genai.GenerativeModel(f\"models/{model}\")\n",
        "                except:\n",
        "                    model_client = genai.GenerativeModel(model)\n",
        "            except:\n",
        "                # If specified model fails, fall back to the working model\n",
        "                model_client = gemini_client\n",
        "                model = gemini_model_name\n",
        "        else:\n",
        "            model_client = gemini_client\n",
        "            model = gemini_model_name if gemini_model_name else \"gemini-pro\"\n",
        "            \n",
        "        if not model_client:\n",
        "            return \"‚ùå Gemini client not initialized. Please set up Gemini API key in .env file.\"\n",
        "            \n",
        "        full_prompt = f\"{system_prompt}\\n\\nUser request: {prompt}\\n\\nRespond in JSON format:\"\n",
        "        response = model_client.generate_content(full_prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        # Provide helpful error messages\n",
        "        if \"404\" in error_msg and \"not found\" in error_msg.lower():\n",
        "            return f\"‚ùå Gemini model '{model}' not found. Available models may have changed.\\n   Re-run the setup cell to auto-detect available models.\\n   Or check: https://ai.google.dev/gemini-api/docs/models/gemini\"\n",
        "        elif \"401\" in error_msg or \"invalid\" in error_msg.lower() and \"key\" in error_msg.lower():\n",
        "            return f\"‚ùå Gemini API key is invalid. Please check your .env file:\\n   GEMINI_API_KEY=your-valid-key-here\\n   Get a new key from: https://aistudio.google.com/app/apikey\"\n",
        "        else:\n",
        "            return f\"‚ùå Error calling Gemini: {error_msg}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User request: List all users in the database\n",
            "\n",
            "LLM Analysis (openai):\n",
            "‚ùå OpenAI API key is invalid or expired. Please check your .env file:\n",
            "   OPENAI_API_KEY=your-valid-key-here\n",
            "   Get a new key from: https://platform.openai.com/api-keys\n",
            "\n",
            "Could not parse JSON from LLM response: Expecting value: line 1 column 1 (char 0)\n",
            "Using direct interpretation.\n",
            "Final result: ‚ùå OpenAI API key is invalid or expired. Please check your .env file:\n",
            "   OPENAI_API_KEY=your-valid-key-here\n",
            "   Get a new key from: https://platform.openai.com/api-keys\n",
            "User request: Create a Python script that calculates fibonacci numbers\n",
            "\n",
            "LLM Analysis (gemini):\n",
            "```json\n",
            "{\n",
            "  \"reasoning\": \"The user wants a Python script to calculate Fibonacci numbers. I will define a Python function for Fibonacci calculation and then use `code_write_file` to save it to a file named `fibonacci.py`.\",\n",
            "  \"tool\": \"code_write_file\",\n",
            "  \"server\": \"code\",\n",
            "  \"arguments\": {\n",
            "    \"file_path\": \"fibonacci.py\",\n",
            "    \"content\": \"def fibonacci(n):\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    else:\\n        list_fib = [0, 1]\\n        while len(list_fib) < n:\\n            next_fib = list_fib[-1] + list_fib[-2]\\n            list_fib.append(next_fib)\\n        return list_fib\\n\\nif __name__ == '__main__':\\n    num_terms = 10  # You can change this number\\n    fib_sequence = fibonacci(num_terms)\\n    print(f\\\"Fibonacci sequence up to {num_terms} terms: {fib_sequence}\\\")\"\n",
            "  },\n",
            "  \"response\": \"A Python script named `fibonacci.py` has been created. It contains a function to calculate Fibonacci numbers.\"\n",
            "}\n",
            "```\n",
            "\n",
            "Executing: code.write_file with arguments: {'file_path': 'fibonacci.py', 'content': 'def fibonacci(n):\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    else:\\n        list_fib = [0, 1]\\n        while len(list_fib) < n:\\n            next_fib = list_fib[-1] + list_fib[-2]\\n            list_fib.append(next_fib)\\n        return list_fib\\n\\nif __name__ == \\'__main__\\':\\n    num_terms = 10  # You can change this number\\n    fib_sequence = fibonacci(num_terms)\\n    print(f\"Fibonacci sequence up to {num_terms} terms: {fib_sequence}\")'}\n",
            "\n",
            "Tool result:\n",
            "Successfully wrote 458 characters to fibonacci.py\n",
            "\n",
            "Final result: Successfully wrote 458 characters to fibonacci.py\n"
          ]
        }
      ],
      "source": [
        "async def execute_api_agent_request(user_prompt: str, provider: str = \"openai\", model: str = None):\n",
        "    \"\"\"Execute a user request using API-based agent (OpenAI or Gemini).\"\"\"\n",
        "    print(f\"User request: {user_prompt}\\n\")\n",
        "    \n",
        "    # Select provider and model\n",
        "    if provider.lower() == \"openai\":\n",
        "        if not openai_client:\n",
        "            return \"‚ùå OpenAI client not initialized. Please provide an OpenAI API key in the setup cell above.\"\n",
        "        if model is None:\n",
        "            model = \"gpt-4o-mini\"\n",
        "        llm_response = openai_agent(user_prompt, model=model)\n",
        "    elif provider.lower() == \"gemini\":\n",
        "        if not gemini_client:\n",
        "            return \"‚ùå Gemini client not initialized. Please provide a Gemini API key in the setup cell above.\"\n",
        "        if model is None:\n",
        "            # Use the model that worked during initialization\n",
        "            model = gemini_model_name if gemini_model_name else \"gemini-pro\"\n",
        "        llm_response = gemini_agent(user_prompt, model=model)\n",
        "    else:\n",
        "        return f\"‚ùå Unknown provider: {provider}. Use 'openai' or 'gemini'.\"\n",
        "    \n",
        "    print(f\"LLM Analysis ({provider}):\\n{llm_response}\\n\")\n",
        "    \n",
        "    # Try to parse JSON response\n",
        "    try:\n",
        "        import re\n",
        "        json_match = re.search(r'\\{.*\\}', llm_response, re.DOTALL)\n",
        "        if json_match:\n",
        "            decision = json.loads(json_match.group())\n",
        "        else:\n",
        "            # Try direct JSON parse\n",
        "            decision = json.loads(llm_response)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not parse JSON from LLM response: {e}\")\n",
        "        print(\"Using direct interpretation.\")\n",
        "        decision = {\"response\": llm_response, \"tool\": None}\n",
        "    \n",
        "    # Execute tool if specified\n",
        "    # Handle both single tool and list of tools\n",
        "    tool_data = decision.get(\"tool\")\n",
        "    server = decision.get(\"server\")\n",
        "    arguments = decision.get(\"arguments\")\n",
        "    \n",
        "    # If tool is a list, take the first one (or handle multiple tools)\n",
        "    if isinstance(tool_data, list):\n",
        "        if len(tool_data) > 0:\n",
        "            tool_name = tool_data[0]\n",
        "            print(f\"‚ö† LLM returned multiple tools, using first: {tool_name}\")\n",
        "        else:\n",
        "            return decision.get(\"response\", \"No action taken\")\n",
        "    elif tool_data:\n",
        "        tool_name = tool_data\n",
        "    else:\n",
        "        return decision.get(\"response\", \"No action taken\")\n",
        "    \n",
        "    # Validate we have all required fields\n",
        "    if not tool_name or not server or not arguments:\n",
        "        return decision.get(\"response\", \"No action taken - missing tool, server, or arguments\")\n",
        "    \n",
        "    # Ensure tool_name is a string\n",
        "    if not isinstance(tool_name, str):\n",
        "        tool_name = str(tool_name)\n",
        "    \n",
        "    # Strip server prefix from tool name if present (e.g., \"document_create_document\" -> \"create_document\")\n",
        "    # The LLM might return the prefixed name, but we need the actual tool name\n",
        "    if isinstance(tool_name, str) and tool_name.startswith(f\"{server}_\"):\n",
        "        actual_tool_name = tool_name[len(f\"{server}_\"):]\n",
        "    else:\n",
        "        actual_tool_name = tool_name\n",
        "    \n",
        "    # Map server name to params\n",
        "    server_map = {\n",
        "        \"code\": CODE_SERVER_PARAMS,\n",
        "        \"database\": DB_SERVER_PARAMS,\n",
        "        \"document\": DOC_SERVER_PARAMS\n",
        "    }\n",
        "    \n",
        "    if server in server_map:\n",
        "        print(f\"Executing: {server}.{actual_tool_name} with arguments: {arguments}\\n\")\n",
        "        try:\n",
        "            result = await call_mcp_tool(server_map[server], actual_tool_name, arguments)\n",
        "            print(f\"Tool result:\\n{result}\\n\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error executing tool: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            return error_msg\n",
        "    else:\n",
        "        return f\"Unknown server: {server}\"\n",
        "\n",
        "# Example usage (uncomment to test):\n",
        "if openai_client:\n",
        "    result = await execute_api_agent_request(\"List all users in the database\", provider=\"openai\")\n",
        "    print(f\"Final result: {result}\")\n",
        "\n",
        "if gemini_client:\n",
        "    result = await execute_api_agent_request(\"Create a Python script that calculates fibonacci numbers\", provider=\"gemini\")\n",
        "    print(f\"Final result: {result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Agent Function\n",
        "\n",
        "A unified function that lets you choose which LLM provider to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def intelligent_agent(user_prompt: str, provider: str = \"ollama\", model: str = None):\n",
        "    \"\"\"\n",
        "    Intelligent agent that uses LLM to understand requests and execute MCP tools.\n",
        "    \n",
        "    Args:\n",
        "        user_prompt: Natural language request from user\n",
        "        provider: \"ollama\", \"openai\", or \"gemini\"\n",
        "        model: Model name (optional, uses defaults if not provided)\n",
        "    \n",
        "    Returns:\n",
        "        Result of the agent's actions\n",
        "    \"\"\"\n",
        "    print(f\"ü§ñ Using {provider.upper()} agent\\n\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if provider.lower() == \"ollama\":\n",
        "        if not OLLAMA_AVAILABLE:\n",
        "            return \"Ollama is not available. Install with: pip install ollama\"\n",
        "        if model is None:\n",
        "            model = \"llama3\"\n",
        "        return await execute_ollama_agent_request(user_prompt, model=model)\n",
        "    elif provider.lower() in [\"openai\", \"gemini\"]:\n",
        "        if model is None:\n",
        "            model = \"gpt-4o-mini\" if provider.lower() == \"openai\" else \"gemini-pro\"\n",
        "        return await execute_api_agent_request(user_prompt, provider=provider, model=model)\n",
        "    else:\n",
        "        return f\"Unknown provider: {provider}. Use 'ollama', 'openai', or 'gemini'\"\n",
        "\n",
        "# Example usage:\n",
        "print(\"Ready to use intelligent agent!\")\n",
        "print(\"\\nüìã Available providers:\")\n",
        "if OLLAMA_AVAILABLE:\n",
        "    print(\"  ‚úì Ollama (local)\")\n",
        "if openai_client:\n",
        "    print(\"  ‚úì OpenAI\")\n",
        "if gemini_client:\n",
        "    print(f\"  ‚úì Gemini (using {gemini_model_name if gemini_model_name else 'default'})\")\n",
        "if not any([OLLAMA_AVAILABLE, openai_client, gemini_client]):\n",
        "    print(\"  ‚ö† No providers available - set up at least one in the cells above\")\n",
        "\n",
        "print(\"\\nüí° Example commands:\")\n",
        "print(\"  - 'List all users in the database'\")\n",
        "print(\"  - 'Create a document called notes.txt with some content'\")\n",
        "print(\"  - 'Write a Python function to calculate factorial'\")\n",
        "print(\"  - 'Search for documents containing the word MCP'\")\n",
        "print(\"\\nüìù Usage:\")\n",
        "print(\"  result = await intelligent_agent('your request', provider='ollama'|'openai'|'gemini')\")\n",
        "print(\"\\n‚ö† Important: Use the provider you set up above!\")\n",
        "print(\"  - If you set up Gemini, use provider='gemini'\")\n",
        "print(\"  - If you set up OpenAI, use provider='openai'\")\n",
        "print(\"  - If you have Ollama, use provider='ollama'\")\n",
        "print(\"\\nUncomment below to test:\")\n",
        "# result = await intelligent_agent(\"List all database tables\", provider=\"gemini\")  # Use the provider you set up!\n",
        "# print(f\"\\nFinal Result:\\n{result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Multi-Step Workflow\n",
        "\n",
        "Demonstrates how the agent can handle complex, multi-step requests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def multi_step_workflow_example(provider: str = \"ollama\"):\n",
        "    \"\"\"Example of a complex multi-step workflow using the intelligent agent.\"\"\"\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"Multi-Step Workflow Example\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    \n",
        "    steps = [\n",
        "        \"Query the database to get the count of users\",\n",
        "        \"Create a Python script that prints 'Hello from MCP Agent'\",\n",
        "        \"Create a document summarizing the database query results\"\n",
        "    ]\n",
        "    \n",
        "    for i, step in enumerate(steps, 1):\n",
        "        print(f\"Step {i}: {step}\")\n",
        "        result = await intelligent_agent(step, provider=provider)\n",
        "        print(f\"Result: {result}\\n\")\n",
        "        print(\"-\" * 60)\n",
        "        print()\n",
        "\n",
        "# Uncomment to run:\n",
        "# await multi_step_workflow_example(provider=\"ollama\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "\n",
        "1. **Ollama Integration**: Run LLMs locally without API keys\n",
        "2. **OpenAI Integration**: Use GPT models with API keys\n",
        "3. **Gemini Integration**: Use Google's Gemini models with API keys\n",
        "4. **MCP Tool Integration**: All LLMs can use MCP servers to perform actions\n",
        "5. **Intelligent Agent**: Unified interface for all providers\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try different models and providers\n",
        "- Extend the agent to handle multi-turn conversations\n",
        "- Add more sophisticated tool selection logic\n",
        "- Implement error handling and retry logic\n",
        "- Add conversation memory/history\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
